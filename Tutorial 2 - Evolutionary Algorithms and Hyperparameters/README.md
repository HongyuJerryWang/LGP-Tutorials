# LGP Tutorial 2: Evolutionary Algorithms and Hyperparameters

> *Choosing a suitable evolutionary algorithm and adjusting the hyperparameters for your dataset

## Setup

We will continue to use the setup we created in Tutorial 1, and this tutorial will be performed in the LGP-Tutorial directory, too.

This tutorial does not include programming. We'll perform LGP on a synthetic symbolic regression benchmark, the Pagie-1 (![f(x0, x1) = 1 / (1 + x0 ^ -4) + 1 / (1 + x1 ^ -4)][math1.jpg "(f(x0, x1) = 1 / (1 + x0 ^ -4) + 1 / (1 + x1 ^ -4)"], defined by Pagie & Hogeweg in 1997). Which looks like

![Pagie-1][Pagie-1.png "Pagie-1.png"]

Please download the Pagie-1.csv file from this repository. This original dataset can be found [here](https://github.com/PonyGE/PonyGE2/blob/master/datasets/Paige1/Train.txt).

Please download the Pagie-1-1.json file from this repository. This configuration file mostly relies on the default hyperparameter values and only specifies the minimum information needed.

```
{
    "operations": [
        "lgp.lib.operations.Addition",
        "lgp.lib.operations.Subtraction",
        "lgp.lib.operations.Multiplication"
    ],
    "constants": [0.0, 1.0],
    "numFeatures": 2
}
```

We'll work from here.

## Run the default configuration

Now let's just run our configuration on our dataset.

```
kotlin -cp build/libs/LGP-Tutorial.jar: lgp.tutorial.linearRegression.Main Pagie-1-1.json Pagie-1.csv
```

## Experiment with the hyperparameters

We examine the output to see the performance. I get a best fitness of 0.41.

If we inspect the expected and the actual values in testcase.csv, chances are that the program isn't doing a very good job. Let's take a look at the statistics in the results.csv file generated by the program. The file is named result1.csv in the repository.

Hmm, we can quickly spot a problem, the best fitness has never improved.

![Pagie-1-1][Pagie-1-1.jpg "Best Fitness Line Graph for Pagie-1-1.json"]

The fact is, the default generation number, 50, is too small for most of the problems, let's make the generation number 1000 in our json file and name it Pagie-1-2.json.

```
"generations": 1000
```

It is recommended that we keep the old file as a change may make the performance either better or worse.

```
kotlin -cp build/libs/LGP-Tutorial.jar: lgp.tutorial.linearRegression.Main Pagie-1-2.json Pagie-1.csv
```

After a few tries and inspecting result.csv and testcases.txt, we find that the performance is equally bad, and the best fitness stops improving after the first few generations or has never improved.

![Pagie-1-2][Pagie-1-2.jpg "Best Fitness Line Graph for Pagie-1-2.json"]

Maybe it's the operations we have, as the default set of addition, subtraction and multiplication is quite limited. Let's add division to our operations and see if anything improves.

```
"operations": [
    "lgp.lib.operations.Addition",
    "lgp.lib.operations.Subtraction",
    "lgp.lib.operations.Multiplication",
    "lgp.lib.operations.Division"
]
```

```
kotlin -cp build/libs/LGP-Tutorial.jar: lgp.tutorial.linearRegression.Main Pagie-1-3.json Pagie-1.csv
```

After a few tries, there are some better results. I get a best fitness of 0.23. Looks like division is one of the things we need.

![Pagie-1-3][Pagie-1-3.jpg "Best Fitness Line Graph for Pagie-1-3.json"]

We see that the fitness is improving, which is good, but once in a few hundred generations, which is a bit slow. We can give the program more registers, more variant program length, a bigger population and a greater number of offspring.

```
"initialMinimumProgramLength": 20,
"initialMaximumProgramLength": 100,
"minimumProgramLength": 5,
"maximumProgramLength": 500,
"numCalculationRegisters": 15,
"populationSize": 1000,
"numOffspring": 500
```

Let's see if that improves anything.

```
kotlin -cp build/libs/LGP-Tutorial.jar: lgp.tutorial.linearRegression.Main Pagie-1-4.json Pagie-1.csv
```

After a few tries, we find out that it's actually doing worse. The actual values largely remain 1.5. We've given the program more resources and freedom, but why is it performing worse? After some thinking, we should notice that some of the resources are in fact double-edged swords. Namely, numCalculationRegisters can do harm if too large, as too many calculation registers can make the program harder to connect and make sense. Let's get rid of it and just use the default value.

```
kotlin -cp build/libs/LGP-Tutorial.jar: lgp.tutorial.linearRegression.Main Pagie-1-5.json Pagie-1.csv
```

Chances are that we quickly see a lot of improvement. I get an average difference of about 0.23 between the expected value and the actual value. Please note that I get much worse results sometimes, as every try can differ quite a bit from each other. So indeed numCalculationRegisters is causing the trouble, or at least a part of it. Let's try making the mutations more often and drastic, and see if anything improves.

```
kotlin -cp build/libs/LGP-Tutorial.jar: lgp.tutorial.linearRegression.Main Pagie-1-6.json Pagie-1.csv
```

After a few runs, seems it is not significantly better or worse.

## Experiment with the evolutionary algorithms

Here's something else we can try, as there's another evolutionary algorithm we can try, called Island Migration, which may help the performance by seperating the evolution process into islands, let's try it out. So we add a few more command line arguments, specifying that we want to use Island Migration, the number of Islands we want, the migration interval and the migration size.

```
kotlin -cp build/libs/LGP-Tutorial.jar: lgp.tutorial.linearRegression.Main Pagie-1-6.json Pagie-1.csv IslandMigration 4 10 10
```

After a few runs, we can get more stable performance and the best average difference I get is about 0.14.

## Summary

So we have managed to improve the performance quite a bit along the way, withour doing any programming. The moral of this tutorial is that things like hyperparameters and evolutionary algorithms can really help our program, and tweaking them according to our problem can make a difference. Though in most cases it requires careful consideration of what's happening underneath the hood, along with some patience, it is worth doing.
