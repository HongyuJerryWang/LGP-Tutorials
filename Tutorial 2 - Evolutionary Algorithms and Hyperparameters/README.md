# LGP Tutorial 2: Evolutionary Algorithms and Hyperparameters

> *Choosing a suitable evolutionary algorithm and adjusting the hyperparameters for your dataset

## Setup

Please download **Pagie-1-1.json**, **Pagie-1-2.json**, **Pagie-1-3.json**, **Pagie-1-4.json**, **Pagie-1-5.json**, **Pagie-1-6.json** and **Pagie-1.csv** from this repository, and move a copy of **Main.kt** and **ProblemDefinition.kt** from the previous tutorial, into a sub-directory of **LGP-Tutorials**, e.g. **Tutorial2**.

In **Tutorial2**, compile with:

```
export CLASSPATH=../LGP.jar:../argparser.jar:../xenocom.jar
kotlinc -cp $CLASSPATH -no-stdlib *.kt
```

This tutorial does not include programming. We'll perform LGP on a synthetic symbolic regression benchmark, the Pagie-1 (![f(x0, x1) = 1 / (1 + x0 ^ -4) + 1 / (1 + x1 ^ -4)](https://github.com/HongyuJerryWang/LGP-Tutorials/blob/master/Tutorial%202%20-%20Evolutionary%20Algorithms%20and%20Hyperparameters/math1.jpg "(f(x0, x1) = 1 / (1 + x0 ^ -4) + 1 / (1 + x1 ^ -4)"), defined by Pagie & Hogeweg in 1997). Which looks like

![Pagie-1](https://github.com/HongyuJerryWang/LGP-Tutorials/blob/master/Tutorial%202%20-%20Evolutionary%20Algorithms%20and%20Hyperparameters/Pagie-1.png "Pagie-1.png")

The original dataset can be found [here](https://github.com/PonyGE/PonyGE2/blob/master/datasets/Paige1/Train.txt).

## Running the default configuration

This configuration file mostly relies on the default hyperparameter values and only specifies the minimum information needed:

```
{
    "operations": [
        "lgp.lib.operations.Addition",
        "lgp.lib.operations.Subtraction",
        "lgp.lib.operations.Multiplication"
    ],
    "constants": [0.0, 1.0],
    "featuresBeingCategorical": [false, false],
    "outputsBeingCategorical": [false]
}
```

Now let's just run our configuration on our dataset:

```
kotlin -cp $CLASSPATH:. Main Pagie-1-1.json Pagie-1.csv
```

```
Using specified configuration file (Pagie-1-1.json)...
Using specified data set file (Pagie-1.csv)...
Using default evolutionary algorithm (SteadyState)...
Running...

Dataset details:
numFeatures = 2, numSamples = 676

Configuration:
	initialMinimumProgramLength = 10
	initialMaximumProgramLength = 30
	minimumProgramLength = 10
	maximumProgramLength = 200
	operations = [lgp.lib.operations.Addition, lgp.lib.operations.Subtraction, lgp.lib.operations.Multiplication]
	constantsRate = 0.5
	constants = [0.0, 1.0]
	numCalculationRegisters = 10
	populationSize = 100
	featuresBeingCategorical = [false, false]
	outputsBeingCategorical = [false]
	crossoverRate = 0.5
	microMutationRate = 0.5
	macroMutationRate = 0.5
	generations = 50
	numOffspring = 20
	branchInitialisationRate = 0.0
	stoppingCriterion = 0.0

Results:
Run 1 (best fitness = 0.4113460478456804)
r[0] = 1.0 + r[3]


Stats (last generation only):

generation = 49
bestFitness = 0.4113460478456804
meanFitness = 3.1165545447803955E10
standardDeviationFitness = 3.1009326055907086E11
meanProgramLength = 32.45
meanEffectiveProgramLength = 4.75
```

We examine the output to see the performance. Generally, a lower best fitness means a better performance. I get a best fitness of 0.41.

If we inspect the expected and the actual values in testcase.csv, chances are that the program isn't doing a very good job. Let's take a look at the statistics in the results.csv file generated by the program. The file is named result1.csv in the repository.

Hmm, we can quickly spot a problem, the best fitness has never improved since the first few generations (or not at all in some runs).

![Pagie-1-1](https://github.com/HongyuJerryWang/LGP-Tutorials/blob/master/Tutorial%202%20-%20Evolutionary%20Algorithms%20and%20Hyperparameters/Pagie-1-1.jpg "Best Fitness Line Graph for Pagie-1-1.json")

## Running for more generations

The fact is, the default generation number, 50, is too small for most of the problems, let's use **Pagie-1-2.json**, which specifies a larger number of generations:

```
"generations": 1000
```

It is recommended that we keep the old file as a change may make the performance either better or worse:

```
kotlin -cp $CLASSPATH:. Main Pagie-1-2.json Pagie-1.csv
```

```
Using specified configuration file (Pagie-1-2.json)...
Using specified data set file (Pagie-1.csv)...
Using default evolutionary algorithm (SteadyState)...
Running...

Dataset details:
numFeatures = 2, numSamples = 676

Configuration:
	initialMinimumProgramLength = 10
	initialMaximumProgramLength = 30
	minimumProgramLength = 10
	maximumProgramLength = 200
	operations = [lgp.lib.operations.Addition, lgp.lib.operations.Subtraction, lgp.lib.operations.Multiplication]
	constantsRate = 0.5
	constants = [0.0, 1.0]
	numCalculationRegisters = 10
	populationSize = 100
	featuresBeingCategorical = [false, false]
	outputsBeingCategorical = [false]
	crossoverRate = 0.5
	microMutationRate = 0.5
	macroMutationRate = 0.5
	generations = 1000
	numOffspring = 20
	branchInitialisationRate = 0.0
	stoppingCriterion = 0.0

Results:
Run 1 (best fitness = 0.4113460478456778)
r[7] = r[4] * r[11]
r[5] = f1 + r[3]
r[9] = 1.0 - r[5]
r[4] = r[3] - r[7]
r[8] = r[7] * 1.0
r[11] = r[11] + r[9]
r[10] = r[8] + 1.0
r[0] = r[11] + r[10]
r[2] = f0 - r[4]
r[4] = r[4] + r[4]
r[6] = 0.0 * 1.0
r[10] = 0.0 - r[2]
r[0] = 0.0 * r[10]
r[1] = 1.0 * r[2]
r[8] = 1.0 - f0
r[3] = r[4] * 0.0
r[0] = r[8] - 0.0
r[9] = f0 * 1.0
r[0] = 1.0 + r[9]
r[8] = r[9] - 1.0
r[0] = f0 + r[3]
r[4] = r[6] + 0.0
r[0] = f0 - r[4]
r[0] = f0 - r[6]
r[0] = r[8] + f0
r[11] = r[4] - 0.0
r[7] = r[11] + 0.0
r[10] = 1.0 - f0
r[6] = 0.0 + r[3]
r[3] = r[10] + 1.0
r[5] = r[7] + r[7]
r[1] = f1 - r[8]
r[2] = f1 - f1
r[0] = r[2] + r[8]
r[7] = r[6] + 1.0
r[5] = r[3] * r[5]
r[4] = r[5] + r[7]
r[10] = f0 - r[7]
r[2] = r[4] - f0
r[4] = f1 - r[11]
r[0] = r[3] + r[8]
r[6] = 1.0 + f1
r[11] = f0 - r[4]
r[7] = r[6] * f0
r[2] = r[2] + 0.0
r[5] = r[11] + r[7]
r[7] = r[10] - r[2]
r[8] = r[6] - f1
r[9] = r[7] + 1.0
r[3] = r[2] + r[10]
r[0] = r[8] * r[9]
r[2] = r[6] * f1
r[9] = r[3] * 0.0
r[4] = 0.0 - r[2]
r[10] = 1.0 * f0
r[6] = r[2] + r[9]
r[0] = r[9] * r[6]
r[6] = r[4] + f0
r[7] = r[11] - f1
r[8] = r[6] - r[11]
r[5] = r[5] * r[10]
r[9] = r[8] + 1.0
r[4] = r[5] + r[7]
r[8] = r[4] - r[4]
r[2] = r[8] + 1.0
r[10] = r[5] * r[5]
r[7] = r[9] + r[2]
r[0] = r[7] + r[10]


Stats (last generation only):

generation = 999
bestFitness = 0.4113460478456778
meanFitness = 2.246105737909657E168
standardDeviationFitness = Infinity
meanProgramLength = 236.77
meanEffectiveProgramLength = 50.59
```

After a few tries and inspecting result.csv and testcases.txt, we find that the performance is equally bad, and the best fitness stops improving after the first few generations or has never improved.

![Pagie-1-2](https://github.com/HongyuJerryWang/LGP-Tutorials/blob/master/Tutorial%202%20-%20Evolutionary%20Algorithms%20and%20Hyperparameters/Pagie-1-2.jpg "Best Fitness Line Graph for Pagie-1-2.json")

## Expanding the operations

Maybe it's the operations we have, as the default set of addition, subtraction and multiplication is quite limited. Let's add division to our operations and see if anything improves:

```
"operations": [
    "lgp.lib.operations.Addition",
    "lgp.lib.operations.Subtraction",
    "lgp.lib.operations.Multiplication",
    "lgp.lib.operations.Division"
]
```

```
kotlin -cp $CLASSPATH:. Main Pagie-1-3.json Pagie-1.csv
```

```
Using specified configuration file (Pagie-1-3.json)...
Using specified data set file (Pagie-1.csv)...
Using default evolutionary algorithm (SteadyState)...
Running...

Dataset details:
numFeatures = 2, numSamples = 676

Configuration:
	initialMinimumProgramLength = 10
	initialMaximumProgramLength = 30
	minimumProgramLength = 10
	maximumProgramLength = 200
	operations = [lgp.lib.operations.Addition, lgp.lib.operations.Subtraction, lgp.lib.operations.Multiplication, lgp.lib.operations.Division]
	constantsRate = 0.5
	constants = [0.0, 1.0]
	numCalculationRegisters = 10
	populationSize = 100
	featuresBeingCategorical = [false, false]
	outputsBeingCategorical = [false]
	crossoverRate = 0.5
	microMutationRate = 0.5
	macroMutationRate = 0.5
	generations = 1000
	numOffspring = 20
	branchInitialisationRate = 0.0
	stoppingCriterion = 0.0

Results:
Run 1 (best fitness = 0.2350430237751897)
r[0] = 0.0 / r[11]
r[11] = r[8] / 0.0
r[2] = r[5] + f0
r[8] = 0.0 - r[11]
r[9] = 0.0 * r[9]
r[1] = r[9] / r[8]
r[2] = r[2] * r[9]
r[0] = f1 / r[4]
r[8] = r[5] + f0
r[10] = r[2] * r[2]
r[6] = r[8] - 0.0
r[9] = 0.0 / f1
r[2] = 1.0 / r[10]
r[2] = r[2] * r[9]
r[10] = 1.0 / f1
r[5] = r[2] * 1.0
r[5] = 0.0 + r[5]
r[0] = 0.0 + 1.0
r[8] = r[11] / r[5]
r[6] = r[6] / r[10]
r[6] = f0 + r[6]
r[2] = 1.0 + r[8]
r[2] = r[2] - 0.0
r[2] = r[2] / 1.0
r[10] = 0.0 - r[2]
r[5] = 1.0 - 0.0
r[3] = r[5] - 0.0
r[0] = r[5] + r[6]
r[4] = 1.0 * 1.0
r[11] = r[10] - 0.0
r[11] = r[11] * r[4]
r[2] = r[11] / f0
r[6] = r[2] * r[11]
r[3] = r[6] / r[3]
r[0] = 1.0 + r[3]


Stats (last generation only):

generation = 999
bestFitness = 0.2350430237751897
meanFitness = 3.9999848000290915E250
standardDeviationFitness = Infinity
meanProgramLength = 238.19
meanEffectiveProgramLength = 49.84
```

After a few tries, there are some better results. I get a best fitness of 0.24. Looks like division is one of the things we need.

![Pagie-1-3](https://github.com/HongyuJerryWang/LGP-Tutorials/blob/master/Tutorial%202%20-%20Evolutionary%20Algorithms%20and%20Hyperparameters/Pagie-1-3.jpg "Best Fitness Line Graph for Pagie-1-3.json")

## Searching deeper and wider into the search space

We see that the fitness is improving, which is good, but once in a few hundred generations, which is a bit slow. We can give the program more registers, more variant program length, a bigger population and a greater number of offspring. So the program can search deeper and wider into the search space:

```
"initialMinimumProgramLength": 20,
"initialMaximumProgramLength": 100,
"minimumProgramLength": 5,
"maximumProgramLength": 500,
"numCalculationRegisters": 15,
"populationSize": 1000,
"numOffspring": 50
```

Let's see if that improves anything:

```
kotlin -cp $CLASSPATH:. Main Pagie-1-4.json Pagie-1.csv
```

```
Using specified configuration file (Pagie-1-4.json)...
Using specified data set file (Pagie-1.csv)...
Using default evolutionary algorithm (SteadyState)...
Running...

Dataset details:
numFeatures = 2, numSamples = 676

Configuration:
	initialMinimumProgramLength = 20
	initialMaximumProgramLength = 100
	minimumProgramLength = 5
	maximumProgramLength = 500
	operations = [lgp.lib.operations.Addition, lgp.lib.operations.Subtraction, lgp.lib.operations.Multiplication, lgp.lib.operations.Division]
	constantsRate = 0.5
	constants = [0.0, 1.0]
	numCalculationRegisters = 15
	populationSize = 1000
	featuresBeingCategorical = [false, false]
	outputsBeingCategorical = [false]
	crossoverRate = 0.5
	microMutationRate = 0.5
	macroMutationRate = 0.5
	generations = 1000
	numOffspring = 50
	branchInitialisationRate = 0.0
	stoppingCriterion = 0.0

Results:
Run 1 (best fitness = 0.18320053359763033)
r[15] = r[15] - 1.0
r[3] = 1.0 / r[15]
r[13] = 0.0 + r[3]
r[13] = f1 / r[13]
r[4] = f1 * r[13]
r[9] = r[11] + 0.0
r[12] = r[8] + 0.0
r[6] = 1.0 / r[4]
r[10] = r[6] / 0.0
r[12] = r[12] + r[11]
r[16] = r[9] / r[15]
r[13] = 0.0 + r[16]
r[1] = 1.0 - r[10]
r[4] = r[14] + 1.0
r[9] = r[4] + 0.0
r[3] = 0.0 / r[9]
r[8] = r[12] / r[4]
r[5] = r[13] + r[8]
r[7] = 1.0 - f1
r[11] = r[5] / r[3]
r[4] = r[11] * 1.0
r[3] = 1.0 * r[4]
r[0] = r[3] / r[7]


Stats (last generation only):

generation = 999
bestFitness = 0.18320053359763033
meanFitness = 5.1653157688206026E110
standardDeviationFitness = 1.632599353933384E112
meanProgramLength = 118.7
meanEffectiveProgramLength = 15.794
```

After a few tries, we find out the best fitness might differ quite a bit from run to run, but is in general better, as I get a best fitness of 0.18.

![Pagie-1-4](https://github.com/HongyuJerryWang/LGP-Tutorials/blob/master/Tutorial%202%20-%20Evolutionary%20Algorithms%20and%20Hyperparameters/Pagie-1-4.jpg "Best Fitness Line Graph for Pagie-1-4.json")

It should be noted that searching too deep or wide into the search space may take a painfully long time with little gain in performance, so please increase these hyperparameters bit by bit and observe the performance. Also, the greater the numCalculationRegisters, the bigger the search space, and if the search space is too big, the program might actually do worse, so please treat this hyperparameter with caution.

## Changing the mutation rates

Let's try making the mutations more often and drastic, and see if anything improves:

```
"crossoverRate": 0.8,
"microMutationRate": 0.8,
"macroMutationRate": 0.8
```

```
kotlin -cp $CLASSPATH:. Main Pagie-1-5.json Pagie-1.csv
```

```
Using specified configuration file (Pagie-1-5.json)...
Using specified data set file (Pagie-1.csv)...
Using default evolutionary algorithm (SteadyState)...
Running...

Dataset details:
numFeatures = 2, numSamples = 676

Configuration:
	initialMinimumProgramLength = 20
	initialMaximumProgramLength = 100
	minimumProgramLength = 5
	maximumProgramLength = 500
	operations = [lgp.lib.operations.Addition, lgp.lib.operations.Subtraction, lgp.lib.operations.Multiplication, lgp.lib.operations.Division]
	constantsRate = 0.5
	constants = [0.0, 1.0]
	numCalculationRegisters = 15
	populationSize = 1000
	featuresBeingCategorical = [false, false]
	outputsBeingCategorical = [false]
	crossoverRate = 0.8
	microMutationRate = 0.8
	macroMutationRate = 0.8
	generations = 1000
	numOffspring = 50
	branchInitialisationRate = 0.0
	stoppingCriterion = 0.0

Results:
Run 1 (best fitness = 0.18320053053760713)
r[14] = r[15] * r[7]
r[3] = f1 - r[14]
r[15] = r[5] * r[3]
r[0] = r[15] - r[11]
r[7] = r[4] + 1.0
r[12] = f0 + f0
r[6] = 1.0 - r[7]
r[13] = 0.0 - r[6]
r[13] = r[12] * r[13]
r[4] = r[13] - r[13]
r[6] = 1.0 + r[4]
r[14] = r[6] - 0.0
r[16] = 1.0 / f1
r[10] = f1 / r[16]
r[1] = r[14] + r[6]
r[13] = f1 - 0.0
r[12] = 1.0 + r[10]
r[11] = f1 / r[12]
r[0] = r[13] - r[11]


Stats (last generation only):

generation = 999
bestFitness = 0.18320053053760713
meanFitness = 1.8721627753622668E104
standardDeviationFitness = 5.917337623149406E105
meanProgramLength = 146.225
meanEffectiveProgramLength = 21.277
```

After a few runs, seems it is not significantly better or worse, and I get a best fitness of 0.18.

![Pagie-1-5](https://github.com/HongyuJerryWang/LGP-Tutorials/blob/master/Tutorial%202%20-%20Evolutionary%20Algorithms%20and%20Hyperparameters/Pagie-1-5.jpg "Best Fitness Line Graph for Pagie-1-5.json")

In my experiments, making the mutations less often and drastic seemed to yield similar results.

## Experimenting with the evolutionary algorithms

Here's something else we can try, as there's another evolutionary algorithm we can try, called Island Migration, which may help the performance by seperating the evolution process into islands. In Island Migration, each island runs its own LGP like the normal LGP we just performed, for a specified number of generations, then a specified number of individuals migrate to between islands. This process is repeated until the maximum number of generations is reached or a solution with a perfect fitness (i.e. 0) is found. This numberOfRunsalgorithm tends to maintain a greater diversity of individuals than our normal evolutionary algorithm (i.e. Steady State). Let's try it out. So we add a few more command line arguments, specifying that we want to use Island Migration, the number of Islands we want, the migration interval and the migration size. For example, here we have 4 islands, migration happens every 10 generations, and each time 10 individuals migrate from one island to another:

```
kotlin -cp $CLASSPATH:. Main Pagie-1-5.json Pagie-1.csv IslandMigration 4 10 10
```

```
Using specified configuration file (Pagie-1-5.json)...
Using specified data set file (Pagie-1.csv)...
Using specified evolutionary algorithm (IslandMigration)...
Running...

Dataset details:
numFeatures = 2, numSamples = 676

Configuration:
	initialMinimumProgramLength = 20
	initialMaximumProgramLength = 100
	minimumProgramLength = 5
	maximumProgramLength = 500
	operations = [lgp.lib.operations.Addition, lgp.lib.operations.Subtraction, lgp.lib.operations.Multiplication, lgp.lib.operations.Division]
	constantsRate = 0.5
	constants = [0.0, 1.0]
	numCalculationRegisters = 15
	populationSize = 1000
	featuresBeingCategorical = [false, false]
	outputsBeingCategorical = [false]
	crossoverRate = 0.8
	microMutationRate = 0.8
	macroMutationRate = 0.8
	generations = 1000
	numOffspring = 50
	branchInitialisationRate = 0.0
	stoppingCriterion = 0.0

Results:
Run 1 (best fitness = 0.1532264850180447)
r[8] = r[12] + 1.0
r[10] = 0.0 - r[8]
r[14] = r[16] - r[10]
r[15] = r[9] - 1.0
r[0] = f1 * r[7]
r[9] = 1.0 - f0
r[16] = r[14] / f0
r[4] = r[9] * f1
r[12] = r[11] / 1.0
r[8] = 1.0 / r[15]
r[2] = 0.0 - r[16]
r[16] = r[2] * r[15]
r[10] = r[2] * 1.0
r[11] = r[16] - r[4]
r[12] = r[12] / r[8]
r[4] = 1.0 - r[11]
r[3] = r[12] - r[9]
r[15] = r[3] - r[10]
r[8] = r[4] + r[9]
r[9] = 0.0 - r[14]
r[0] = 1.0 / 1.0
r[9] = r[8] - r[9]
r[3] = 0.0 - r[15]
r[8] = 1.0 / r[3]
r[15] = 0.0 / r[9]
r[11] = r[2] - r[15]
r[2] = 1.0 + f1
r[5] = f0 * r[8]
r[4] = 0.0 - r[11]
r[8] = r[2] - r[5]
r[2] = r[4] + r[8]
r[2] = f1 / r[2]
r[0] = r[2] + 1.0


Stats (last generation only):

generation = 1000
bestFitness = 0.1532264850180447
meanFitness = 3.946382215254556E40
standardDeviationFitness = 7.362058040493826E41
meanProgramLength = 144.6235
meanEffectiveProgramLength = 19.52875
```

After a few runs, I get a best fitness of 0.15.

![Pagie-1-5 Island Migration](https://github.com/HongyuJerryWang/LGP-Tutorials/blob/master/Tutorial%202%20-%20Evolutionary%20Algorithms%20and%20Hyperparameters/Pagie-1-5-IslandMigration.jpg "Best Fitness Line Graph for Pagie-1-5.json with Island Migration")

## Early stopping

We can see in the graph above that the best fitness reached 0.13 at one point, but went back up again, maybe we can tell the program to stop once the best fitness drops below 0.14:

```
"stoppingCriterion": 0.14
```

```
kotlin -cp $CLASSPATH:. Main Pagie-1-6.json Pagie-1.csv IslandMigration 4 10 10
```

```
Using specified configuration file (Pagie-1-6.json)...
Using specified data set file (Pagie-1.csv)...
Using specified evolutionary algorithm (IslandMigration)...
Running...

Dataset details:
numFeatures = 2, numSamples = 676

Configuration:
	initialMinimumProgramLength = 20
	initialMaximumProgramLength = 100
	minimumProgramLength = 5
	maximumProgramLength = 500
	operations = [lgp.lib.operations.Addition, lgp.lib.operations.Subtraction, lgp.lib.operations.Multiplication, lgp.lib.operations.Division]
	constantsRate = 0.5
	constants = [0.0, 1.0]
	numCalculationRegisters = 15
	populationSize = 1000
	featuresBeingCategorical = [false, false]
	outputsBeingCategorical = [false]
	crossoverRate = 0.8
	microMutationRate = 0.8
	macroMutationRate = 0.8
	generations = 1000
	numOffspring = 50
	branchInitialisationRate = 0.0
	stoppingCriterion = 0.14

Results:
Run 1 (best fitness = 0.12073984877432739)
r[3] = r[10] * f0
r[15] = r[8] - 0.0
r[4] = r[2] * r[15]
r[2] = 1.0 / r[16]
r[0] = r[3] * f1
r[6] = 1.0 / f0
r[0] = r[6] / r[2]
r[12] = r[2] / r[4]
r[10] = r[12] - 1.0
r[14] = r[10] - f0
r[4] = 1.0 * r[14]
r[12] = r[4] * f0
r[9] = r[2] - r[12]
r[11] = 1.0 / r[9]
r[0] = 1.0 + r[11]


Stats (last generation only):

generation = 330
bestFitness = 0.12073984877432739
meanFitness = 9.615378850117503E36
standardDeviationFitness = 6.080539333893709E38
meanProgramLength = 88.28975
meanEffectiveProgramLength = 10.39825
```

After a few runs, I get a best fitness of 0.12.

![Pagie-1-6 Island Migration](https://github.com/HongyuJerryWang/LGP-Tutorials/blob/master/Tutorial%202%20-%20Evolutionary%20Algorithms%20and%20Hyperparameters/Pagie-1-6-IslandMigration.jpg "Best Fitness Line Graph for Pagie-1-6.json with Island Migration")

## Summary

So we have managed to improve the performance quite a bit along the way, withour doing any programming.

It should be noted that LGP is stochastic, therefore different runs will give different results. You may need to do 3 to 5 runs to get an idea of how good a configuration is. For the clarity of this tutorial, the configuration files use single runs. Specify:

```
"numberOfRuns": 10
```

in the configuration file to perform 10 runs in one go.

The moral of this tutorial is that things like hyperparameters and evolutionary algorithms can really help our program, and tweaking them according to our problem can make a difference. Though in most cases it requires careful consideration of what's happening underneath the hood, along with some patience, it is worth doing.
