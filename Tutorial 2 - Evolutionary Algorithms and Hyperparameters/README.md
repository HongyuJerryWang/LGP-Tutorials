# LGP Tutorial 2: Evolutionary Algorithms and Hyperparameters

> *Choosing a suitable evolutionary algorithm and adjusting the hyperparameters for your dataset

## Setup

We will continue to use the program we used in Tutorial 1, and this tutorial will be performed in the LGP-Tutorial directory, too.

This tutorial is non-programming. We'll perform LGP on a synthetic symbolic regression benchmark, the Pagie-1 (f(x0, x1) = 1 / (1 + x0 ^ -4) + 1 / (1 + x1 ^ -4), defined by Pagie & Hogeweg in 1997). Please download the Pagie-1.csv file from this repository. This original dataset can be found [here](https://github.com/PonyGE/PonyGE2/blob/master/datasets/Paige1/Train.txt).

Please download the Pagie-1-1.json file from this repository. This configuration file mostly relies on the default hyperparameter values and only specifies the minimum information needed. We'll work from here.

It is certainly helpful to have some background knowledge on how LGP works as an algorithm. Please refer to [this page](http://lgp-tutorial.jedsimson.co.nz/lgp/lgp.html) by Jed Simson, if you haven't done so in Tutorial 1.

## Run the default configuration

Now let's just run our configuration on our dataset.

```
kotlin -cp build/libs/LGP-Tutorial.jar: lgp.tutorial.linearRegression.Main Pagie-1-1.json Pagie-1.csv
```

## Experiment with the hyperparameters

I get an average difference of about 0.43 between the expected value and the actual value.

If we inspect the expected and the actual values, chances are that the program isn't doing a very good job. Let's take a look at the statistics in the results.csv file generated by the program. The file is named result1.csv in the repository.

Hmm, we can quickly spot a problem, the best fitness hasn't improved since generation 2. The fact is, the default generation number, 50, is too small for most of the problems, let's make the generation number 1000 in our json file and name it Pagie-1-2.json. It is recommended that we keep the old file as a change may make the performance either better or worse.

```
kotlin -cp build/libs/LGP-Tutorial.jar: lgp.tutorial.linearRegression.Main Pagie-1-2.json Pagie-1.csv
```

After a few tries and inspecting the results.csv file, we find that the performance is equally bad, and the fitness stops improving after the first few generations. Maybe it's the operations we have, as there're a lot of things we just can't do with addition, subtraction and multiplication, let's add division to our operations and see if anything improves.

```
kotlin -cp build/libs/LGP-Tutorial.jar: lgp.tutorial.linearRegression.Main Pagie-1-3.json Pagie-1.csv
```

After a few tries, there are some better results. I get an average difference of about 0.30 between the expected value and the actual value. Looks like division is one of the things we need. Let's inspect the results.csv file again. We see that the fitness is improving, which is good, but once in a few hundred generations, which is a bit slow. We can give the program more resources and freedom, and see if that improves anything.

```
kotlin -cp build/libs/LGP-Tutorial.jar: lgp.tutorial.linearRegression.Main Pagie-1-4.json Pagie-1.csv
```

After a few tries, we find out that it's actually doing worse. The actual values largely remain 1.5. We've given the program more resources and freedom, but why is it performing worse? After some thinking, we should notice that some of the resources are in face double-edged swords. Namely, numCalculationRegisters can do harm if too large, as too many calculation registers can make the program harder to connect and make sense. Let's get rid of it and just use the default value.

```
kotlin -cp build/libs/LGP-Tutorial.jar: lgp.tutorial.linearRegression.Main Pagie-1-5.json Pagie-1.csv
```

Chances are that we quickly see a lot of improvement. I get an average difference of about 0.23 between the expected value and the actual value. Please note that I get much worse results sometimes, as every try can differ quite a bit from each other. So indeed numCalculationRegisters is causing the trouble, or at least a part of it. Let's try making the mutations more often and drastic, and see if anything improves.

```
kotlin -cp build/libs/LGP-Tutorial.jar: lgp.tutorial.linearRegression.Main Pagie-1-6.json Pagie-1.csv
```

After a few runs, seems it is not significantly better or worse.

## Experiment with the evolutionary algorithms

Here's something else we can try, as there's another evolutionary algorithm we can try, called Island Migration, which may help the performance by seperating the evolution process into islands, let's try it out. So we add a few more command line arguments, specifying that we want to use Island Migration, the number of Islands we want, the migration interval and the migration size.

```
kotlin -cp build/libs/LGP-Tutorial.jar: lgp.tutorial.linearRegression.Main Pagie-1-6.json Pagie-1.csv IslandMigration 4 10 10
```

After a few runs, we can get more stable performance and the best average difference I get is about 0.14.

## Summary

So we have managed to improve the performance quite a bit along the way, withour doing any programming. The moral of this tutorial is that things like hyperparameters and evolutionary algorithms can really help our program, and tweaking them according to our problem can make a difference. Though in most cases it requires careful consideration of what's happening underneath the hood, along with some patience, it is worth doing.
